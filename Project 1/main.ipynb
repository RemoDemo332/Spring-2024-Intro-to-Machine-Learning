{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1, 1261]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m y_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtra_Y_te\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Split the training data into training and validation sets\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m X_train, X_val, y_train, y_val \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train_flattened\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\n\u001b[0;32m     33\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Train the Random Forest model\u001b[39;00m\n\u001b[0;32m     36\u001b[0m model \u001b[38;5;241m=\u001b[39m RandomForestRegressor(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\model_selection\\_split.py:2657\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_arrays \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one array required as input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2657\u001b[0m arrays \u001b[38;5;241m=\u001b[39m \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2659\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m   2660\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2661\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[0;32m   2662\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\validation.py:514\u001b[0m, in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \n\u001b[0;32m    486\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;124;03m[[1, 2, 3], array([2, 3, 4]), None, <3x1 sparse matrix ...>]\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    513\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[1;32m--> 514\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    460\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1, 1261]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.io import loadmat\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    "    mean_absolute_error,\n",
    "    median_absolute_error,\n",
    "    explained_variance_score\n",
    ")\n",
    "\n",
    "# Load the dataset\n",
    "data = loadmat('./traffic_dataset.mat')  # Adjust the path to your dataset\n",
    "\n",
    "# Prepare the dataset\n",
    "X_train_raw = data['tra_X_tr']  # Assuming shape: (time intervals, sensors, features)\n",
    "X_test_raw = data['tra_X_te']  # Assuming same shape as X_train_raw\n",
    "\n",
    "# Flatten across sensors and features for each time interval\n",
    "X_train_flattened = X_train_raw.reshape(X_train_raw.shape[0], -1)\n",
    "X_test_flattened = X_test_raw.reshape(X_test_raw.shape[0], -1)\n",
    "\n",
    "# Aggregate traffic volumes across sensors for each time interval\n",
    "y_train = np.sum(data['tra_Y_tr'], axis=0)  # Assuming shape: (sensors, time intervals)\n",
    "y_test = np.sum(data['tra_Y_te'], axis=0)\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_flattened, y_train, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train the Random Forest model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions_train = model.predict(X_train)\n",
    "predictions_val = model.predict(X_val)\n",
    "\n",
    "# Calculate and print evaluation metrics\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, predictions_train))\n",
    "rmse_val = np.sqrt(mean_squared_error(y_val, predictions_val))\n",
    "r2_train = r2_score(y_train, predictions_train)\n",
    "r2_val = r2_score(y_val, predictions_val)\n",
    "\n",
    "print(f'Training RMSE: {rmse_train}')\n",
    "print(f'Validation RMSE: {rmse_val}')\n",
    "print(f'Training R²: {r2_train}')\n",
    "print(f'Validation R²: {r2_val}')\n",
    "\n",
    "# Print additional evaluation metrics\n",
    "mae_train = mean_absolute_error(y_train, predictions_train)\n",
    "mae_val = mean_absolute_error(y_val, predictions_val)\n",
    "medae_train = median_absolute_error(y_train, predictions_train)\n",
    "medae_val = median_absolute_error(y_val, predictions_val)\n",
    "evs_train = explained_variance_score(y_train, predictions_train)\n",
    "evs_val = explained_variance_score(y_val, predictions_val)\n",
    "\n",
    "print(f'Training MAE: {mae_train}')\n",
    "print(f'Validation MAE: {mae_val}')\n",
    "print(f'Training Median AE: {medae_train}')\n",
    "print(f'Validation Median AE: {medae_val}')\n",
    "print(f'Training Explained Variance Score: {evs_train}')\n",
    "print(f'Validation Explained Variance Score: {evs_val}')\n",
    "\n",
    "\n",
    "# Select a sample from the validation set to showcase actual vs. predicted traffic volume\n",
    "sample_index = np.random.randint(0, len(y_val))  # Randomly select an index for sample demonstration\n",
    "sample_features = X_val_flattened[sample_index]\n",
    "sample_true_volume = y_val[sample_index]\n",
    "sample_prediction = model.predict([sample_features])[0]\n",
    "\n",
    "print(f\"\\nSample Prediction:\\nActual traffic volume: {sample_true_volume}\")\n",
    "print(f\"Predicted traffic volume: {sample_prediction:.2f}\")\n",
    "\n",
    "# Plot actual vs predicted traffic volume for the sample\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(['Actual Volume', 'Predicted Volume'], [sample_true_volume, sample_prediction], color=['blue', 'orange'])\n",
    "plt.title('Actual vs Predicted Traffic Volume for Selected Sample')\n",
    "plt.show()\n",
    "\n",
    "# Calculate residuals for the validation set and plot a residual plot\n",
    "residuals = y_val - predictions_val\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(predictions_val, residuals)\n",
    "plt.axhline(y=0, color='red', linestyle='--')\n",
    "plt.xlabel('Predicted Traffic Volume')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot for Validation Set')\n",
    "plt.show()\n",
    "\n",
    "# Plotting the feature importances\n",
    "feature_importances_normalized = model.feature_importances_ / np.max(model.feature_importances_)\n",
    "sorted_indices = np.argsort(feature_importances_normalized)[::-1]\n",
    "top_n = 20  # Number of top features to plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title('Top 20 Feature Importances')\n",
    "plt.bar(range(top_n), feature_importances_normalized[sorted_indices[:top_n]], align='center')\n",
    "plt.xticks(range(top_n), sorted_indices[:top_n], rotation=90)\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Normalized Importance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 2017-01-02 00:00, the traffic is predicted to be approximately 15 vehicles.\n",
      "At 2017-01-02 00:15, the traffic is predicted to be approximately 15 vehicles.\n",
      "At 2017-01-02 00:30, the traffic is predicted to be approximately 15 vehicles.\n",
      "At 2017-01-02 00:45, the traffic is predicted to be approximately 12 vehicles.\n",
      "At 2017-01-02 01:00, the traffic is predicted to be approximately 14 vehicles.\n",
      "At 2017-01-02 01:15, the traffic is predicted to be approximately 13 vehicles.\n",
      "At 2017-01-02 01:30, the traffic is predicted to be approximately 7 vehicles.\n",
      "At 2017-01-02 01:45, the traffic is predicted to be approximately 16 vehicles.\n",
      "At 2017-01-02 02:00, the traffic is predicted to be approximately 15 vehicles.\n",
      "At 2017-01-02 02:15, the traffic is predicted to be approximately 16 vehicles.\n",
      "At 2017-01-02 02:30, the traffic is predicted to be approximately 7 vehicles.\n",
      "At 2017-01-02 02:45, the traffic is predicted to be approximately 14 vehicles.\n",
      "At 2017-01-02 03:00, the traffic is predicted to be approximately 13 vehicles.\n",
      "At 2017-01-02 03:15, the traffic is predicted to be approximately 16 vehicles.\n",
      "At 2017-01-02 03:30, the traffic is predicted to be approximately 14 vehicles.\n",
      "At 2017-01-02 03:45, the traffic is predicted to be approximately 2 vehicles.\n",
      "At 2017-01-02 04:00, the traffic is predicted to be approximately 16 vehicles.\n",
      "At 2017-01-02 04:15, the traffic is predicted to be approximately 17 vehicles.\n",
      "At 2017-01-02 04:30, the traffic is predicted to be approximately 13 vehicles.\n",
      "At 2017-01-02 04:45, the traffic is predicted to be approximately 3 vehicles.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 20 is out of bounds for axis 0 with size 20",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m timestamps \u001b[38;5;241m=\u001b[39m [start_datetime \u001b[38;5;241m+\u001b[39m timedelta(minutes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m \u001b[38;5;241m*\u001b[39m i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(sample_size)]\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(sample_size):\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimestamps[i]\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, the traffic is predicted to be approximately \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(\u001b[43mpredicted_traffic\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m vehicles.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m     18\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(timestamps, predicted_traffic, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted Traffic Volume\u001b[39m\u001b[38;5;124m'\u001b[39m, marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 20 is out of bounds for axis 0 with size 20"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffic Flow Prediction Using Random Forest Regression\n",
    "\n",
    "## 1. Introduction\n",
    "Traffic flow prediction is important in modern urban planning and traffic management. Models may help to reduce congestion and plan optimal road routing. In this project, the goal is to utilize machine learning to predict the next 15 minutes of traffic at 36 sensor locations along highways in Northern Virginia & D.C. area.\n",
    "\n",
    "## 2. Problem Description\n",
    "The dataset for this regression problem contains 47 features, including historical traffic volume, time of day, and day of the week, across 36 locations. The challenge is to predict the future traffic volume for these locations, understanding the spatio-temporal patterns in the data.\n",
    "\n",
    "## 3. Data Preparation\n",
    "The dataset is in a MATLAB format, containing over a year of data. Data preparation involved flattening the matrices into vectors and aggregating features.\n",
    "\n",
    "## 4. Benchmarking\n",
    "Due to its ease of use and robustness, I chose random forest as the first benchmarking model. I tuned the hyperparameters to better suit the problem and achieve better results.\n",
    "\n",
    "## 5. Methodology\n",
    "The model's hyperparameters were set with 100 trees and the default depth to balance complexity and performance.\n",
    "\n",
    "## 6. Results\n",
    "The model performed exceptionally well on the training data as well as the validation data, indicated by the RMSE and R² scores. The results from additional metrics also reflected good performance.\n",
    "- Training RMSE: 0.0059\n",
    "- Validation RMSE: 0.0151\n",
    "- Training R²: 0.9983\n",
    "- Validation R²: 0.9889\n",
    "\n",
    "# Residual Plot\n",
    "The residual plot displays residuals on the y and predicted on the x. The points are randomly scattered around the x-axis which is the ideal outcome. The randomness of the errors displayed in this plot indicates that the model is not performing errors in a pattern which would indicate some part of it is performing poorly. There is also no general shape of the errors meaning the predictions are not reliant on the magnitude of the prediction. The even distribution of large and small residuals suggests that the model does not over-predict or under-predict\n",
    "# Feature Importances Plot\n",
    "\n",
    "# Prediction Error Plot\n",
    "\n",
    "# Learning Curves Plot\n",
    "The learning curve plot starts with a high training error which quickly declines as more data is included. This indicates that the model is quickly learning from new data points when they are introduced. After the initial drop, the error plateaus indicatting that after a certain point the model stops improving. The validation curve begins at a low error rate and rises slightly before also plateauing. Even with a small amount of data this suggests the model is performing well. The validation curve has an error rate similar to that of the training data curve indicating that the model is not overfitting on training data.\n",
    "\n",
    "## 7. Evaluation and Discussion\n",
    "Visualizations such as learning curves, residual plots, feature importance, and prediction error histograms were generated. The learning curves indicate a model that performs consistently as more data is introduced. The residual plot and prediction error histogram show that errors are relatively small and centered around zero, which is desirable.\n",
    "\n",
    "[Include code cells that generate the visualizations here]\n",
    "\n",
    "## 8. Conclusions\n",
    "The Random Forest model shows promise in predicting traffic flow. The high R² and low RMSE values indicate a strong fit. Further studies could explore more complex models to capture any remaining variance.\n",
    "\n",
    "## 9. References\n",
    "Liang Zhao, Olga Gkountouna, and Dieter Pfoser, 2019. Spatial Auto-regressive Dependency Interpretable Learning Based on Spatial Topological Constraints."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
